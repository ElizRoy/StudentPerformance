---
title: "StudentPerformance"
output: html_document
date: "2024-01-09"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r erasing, echo =FALSE}
rm(list=ls())
```
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r echo=FALSE}
shh <- suppressPackageStartupMessages
warn <- suppressWarnings
shh(library (knitr))
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x,options) 
  {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

Reading the academic record file into R.

```{r initialize, echo = FALSE}

shh(if (!require(tidyverse)){
  install.packages('tidyverse', dependencies = TRUE)
  shh(library(tidyverse))  
})
shh(library(data.table))

shh(library(dplyr))

library("readxl")
raw_uni<-read_excel("/Users/rerk/Desktop/Data Analytics/Capstone/Coding/allcapstone.xlsx")
names(raw_uni)<-names(raw_uni) %>%
  stringr::str_replace_all("\\s", ".")

str(raw_uni)
raw_uni
```
##Distribution of dataset

```{r distribution raw_uni}

library(ggplot2)
theme_set(theme_classic())
raw_uni
#Writing the raw data into csv file for visualising in tableau
write.csv(raw_uni, file = "raw_uni.csv", row.names = F)

#Distribution by Curriculum
Curr_uni <- raw_uni%>%
 group_by(Curriculum) %>%
  dplyr::summarize(count=n()) 

Curr_uni$Curriculum<-factor(Curr_uni$Curriculum,
 levels=Curr_uni$Curriculum[order(Curr_uni$count)])

warn(q<-ggplot(Curr_uni,aes(x=Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")+
  geom_label(aes(label=count),nudge_x=0,nudge_y=1, check_overlap=TRUE)+
    theme(legend.position="none")+
  labs(y="", x="",title ="Distribution of records"))
q+ coord_flip()

#Distribution by program
Pr_uni <- raw_uni%>%
  group_by(Current.Program) %>%
  dplyr::summarize(count=n()) 

Pr_uni$Current.Program<-factor(Pr_uni$Current.Program,
 levels=Pr_uni$Current.Program[order(Pr_uni$count)])

warn(q<-ggplot(Pr_uni,aes(x=Current.Program,y=count, fill = Current.Program)) +
  geom_bar(stat="identity")+
  geom_label(aes(label=count),nudge_x=0,nudge_y=1, check_overlap=TRUE)+
    theme(legend.position="none")+
  labs(y="", x="",title ="Distribution of records"))
q+ coord_flip()

# Removing b  
# Summary statistics
summary(raw_uni$cgpa)


```






## Feature Engineering

Removing columns that arent pertinant.
1. Column #1 will be removed as it numbers the rows and doesnt add value to data eploration.
2. SAT score columns will be removed as there many empty rows (columns 13:16)
3. Description column will be removed (Column 18)
------4. English, math and Physics columns will be removed (Columns 20, 22, 24)
5. Current Program, Total English  and Transcript columns will also be removed (Columns 9,10,17)

```{r feature_engineering, echo=FALSE}
raw_uni<-as.data.frame(raw_uni)
str(raw_uni)

raw_uni[,c(11,14:16,18:19)]

uni<-raw_uni[,-c(11,14:16,18:19)]

names(uni)<-names(uni) %>%
  stringr::str_replace_all("\\s", ".")

#length(uni$EnglishGrade)
str(uni) 


# Frequency table of Curriculum
table(uni$Curriculum)

#Inorder to analyse the data efficiently, the Curricula with more than 10 records will be used for exploration and processing. These will be American (55 records), British (61 records), Indian (22 records), IB (14 records), MOE (18 records) and SABIS (13 records).

#Selecting only the above Curricula
#uni<-uni%>%
#  filter(Curriculum %in% c("American", "Indian", "British", "SABIS", "MOE", "IB"))

```


# CONVERTING HS SCORES IN ENGLISH, MATH AND PHYSICS TO 4.0 SCALE
```{r HS score conversion}
t<-c("A", "B","A1","B1")

for (i in 1:nrow(uni)){
   uni$Curriculum <- ifelse(uni$Curriculum == "African", "Others", uni$Curriculum)
  uni$EnglishGrade <- ifelse(uni$Curriculum %in% c("Others","African"), uni$EnglishGrade, ifelse(uni$Curriculum =="Indian" & is.element(uni$English , t), 4.00, ifelse(uni$Curriculum=="Indian" & uni$English  > 59, 4.00,ifelse(uni$Curriculum =="Indian"  & uni$English >54, 3.50, ifelse(uni$Curriculum =="Indian" & uni$English >49, 3.00, ifelse(uni$Curriculum =="Indian"  & uni$English >42, 2.50, ifelse(uni$Curriculum =="Indian" & uni$English >34, 2.00,ifelse (uni$Curriculum %in% c("American", "SABIS")& uni$English %in% c("A","A+"), 4.00, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English =="A-", 3.67,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="B+", 3.33,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="B", 3.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English == "B-", 2.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="C+",2.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="C", 2.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English =="C-",1.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English =="D+",1.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="D",1.00,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English=="D-",0.67,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$English %in% (93:100), 4.00, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >89,3.67, ifelse ( uni$Curriculum %in% c("American", "SABIS")  & uni$English >86,3.33,  ifelse ( uni$Curriculum %in% c("American", "SABIS")  & uni$English >82, 3.00,ifelse(uni$Curriculum %in% c("American", "SABIS")  & uni$English >79, 2.67,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >76, 2.33,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >72, 2.00,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >69, 1.67,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >66, 1.33, ifelse(uni$Curriculum %in% c("American", "SABIS")  & uni$English >62, 1.00, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$English >59, 0.67,ifelse(uni$Curriculum=="British" & uni$English == "A",4.00, ifelse(uni$Curriculum =="British" & uni$English %in% c("B","C"), 3.00,ifelse( uni$Curriculum=="British" & uni$English %in% c("D", "E"), 2.0, ifelse(uni$Curriculum=="British" & uni$English %in%c("F","G"),1.00,ifelse( uni$Curriculum=="British" & uni$English >69,4.00,ifelse( uni$Curriculum=="British" & uni$English>64.99,3.69,ifelse( uni$Curriculum=="British" & uni$English>59.99,3.33,ifelse( uni$Curriculum=="British" & uni$English>49.99,3.00,ifelse( uni$Curriculum=="British" & uni$English>44.99,2.33,ifelse( uni$Curriculum=="British" & uni$English>39.99, 2.0,ifelse(uni$Curriculum=="IB" & uni$English >5, 4.00,ifelse(uni$Curriculum=="IB" & uni$English > 4, 3.00,ifelse(uni$Curriculum=="IB" & uni$English >2,2.00,ifelse(uni$Curriculum=="IB" & uni$English == 2, 1.00, ifelse(uni$Curriculum=="MOE" & uni$English >89,4.00,ifelse(uni$Curriculum=="MOE" & uni$English >79,3.00,ifelse(uni$Curriculum=="MOE" & uni$English >69,2.00,ifelse(uni$Curriculum=="MOE" & uni$English >59,1.00, 0.00)))))))))))))))))))))))))))))))))))))))))))))))

uni$MathGrade <- ifelse(uni$Curriculum %in% c("Others","African"), uni$MathGrade, ifelse(uni$Curriculum =="Indian"  & is.element(uni$math , t), 4.00,ifelse(uni$Curriculum =="Indian"   & uni$math  > 59, 4.00, ifelse (uni$Curriculum =="Indian"   & uni$math >54,3.50, ifelse (uni$Curriculum =="Indian"  & uni$math >49,3.00, ifelse (uni$Curriculum =="Indian"   & uni$math >42, 2.50,ifelse (uni$Curriculum =="Indian"   & uni$math >34, 2.00,ifelse (uni$Curriculum %in% c("American", "SABIS")& uni$math %in% c("A","A+"), 4.00, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math =="A-", 3.67,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="B+", 3.33,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="B", 3.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math == "B-", 2.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="C+",2.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="C", 2.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math =="C-",1.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math =="D+",1.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="D",1.00,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math=="D-",0.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$math %in% (93:100), 4.00, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >89,3.67, ifelse (uni$Curriculum %in% c("American", "SABIS")  &uni$math >86,3.33, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >82, 3.00, ifelse(uni$Curriculum %in% c("American", "SABIS")  & uni$math >79, 2.67,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >76, 2.33, ifelse (uni$Curriculum  %in% c("American", "SABIS")  &uni$math >72, 2.00,ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >69, 1.67, ifelse (uni$Curriculum  %in% c("American", "SABIS")  & uni$math >66, 1.33, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >62, 1.00, ifelse (uni$Curriculum %in% c("American", "SABIS")  & uni$math >59, 0.67,ifelse(uni$Curriculum=="British" & uni$math == "A",4.00, ifelse(uni$Curriculum =="British" & uni$math %in% c("B","C"), 3.00,ifelse( uni$Curriculum=="British" & uni$math %in% c("D", "E"), 2.0, ifelse(uni$Curriculum=="British" & uni$math %in%c("F","G"),1.00,ifelse( uni$Curriculum=="British" & uni$math >69,4.00,ifelse( uni$Curriculum=="British" & uni$math>64.99,3.69,ifelse( uni$Curriculum=="British" & uni$math>59.99,3.33,ifelse( uni$Curriculum=="British" & uni$math>49.99,3.00,ifelse( uni$Curriculum=="British" & uni$math>44.99,2.33,ifelse( uni$Curriculum=="British" & uni$math>39.99, 2.0,ifelse(uni$Curriculum=="IB" & uni$math >5, 4.00,ifelse(uni$Curriculum=="IB" & uni$math > 4, 3.00,ifelse(uni$Curriculum=="IB" & uni$math >2,2.00,ifelse(uni$Curriculum=="IB" & uni$math == 2, 1.00,ifelse(uni$Curriculum=="MOE" & uni$math >89,4.00,ifelse(uni$Curriculum=="MOE" & uni$math >79,3.00,ifelse(uni$Curriculum=="MOE" & uni$math >69,2.00,ifelse(uni$Curriculum=="MOE" & uni$math >59,1.00, 0.00)))))))))))))))))))))))))))))))))))))))))))))))

uni$PhysicsGrade <- ifelse(uni$Curriculum%in% c("Others","African"), uni$PhysicsGrade, ifelse(uni$Curriculum=="Indian"   & is.element(uni$Physics, t), 4.00,ifelse(uni$Curriculum=="Indian"   & uni$Physics  > 59, 4.00, ifelse (uni$Curriculum=="Indian"   & uni$Physics >54,3.50, ifelse (uni$Curriculum=="Indian"   & uni$Physics >49,3.00, ifelse (uni$Curriculum=="Indian"   & uni$Physics >42, 2.50,ifelse (uni$Curriculum=="Indian"   & uni$Physics >34, 2.00,ifelse (uni$Curriculum %in% c("American", "SABIS")& uni$Physics %in% c("A","A+"), 4.00, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics =="A-", 3.67,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="B+", 3.33,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="B", 3.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics == "B-", 2.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="C+",2.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="C", 2.0, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics =="C-",1.67, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics =="D+",1.33, ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="D",1.00,ifelse(uni$Curriculum %in% c("American", "SABIS")& uni$Physics=="D-",0.67, ifelse(uni$Curriculum%in% c("American", "SABIS")  & uni$Physics %in% (93:100), 4.00, ifelse(uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>89,3.67, ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>86,3.33, ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>82, 3.00,ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>79, 2.67,ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>76, 2.33,ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>72, 2.00,ifelse (uni$Curriculum%in% c("American", "SABIS")   & uni$Physics>69, 1.67,ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>66, 1.33, ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>62, 1.00, ifelse (uni$Curriculum%in% c("American", "SABIS")  & uni$Physics>59, 0.67,ifelse(uni$Curriculum=="British" & uni$Physics == "A",4.00, ifelse(uni$Curriculum =="British" & uni$Physics %in% c("B","C"), 3.00,ifelse( uni$Curriculum=="British" & uni$Physics %in% c("D", "E"), 2.0, ifelse(uni$Curriculum=="British" & uni$Physics %in%c("F","G"),1.00,ifelse( uni$Curriculum=="British" & uni$Physics >69,4.00,ifelse( uni$Curriculum=="British" & uni$Physics>64.99,3.69,ifelse( uni$Curriculum=="British" & uni$Physics>59.99,3.33,ifelse( uni$Curriculum=="British" & uni$Physics>49.99,3.00,ifelse( uni$Curriculum=="British" & uni$Physics>44.99,2.33,ifelse( uni$Curriculum=="British" & uni$Physics>39.99, 2.0,ifelse(uni$Curriculum=="IB" & uni$Physics >5, 4.00,ifelse(uni$Curriculum=="IB" & uni$Physics > 4, 3.00,ifelse(uni$Curriculum=="IB" & uni$Physics >2,2.00,ifelse(uni$Curriculum=="IB" & uni$Physics == 2, 1.00,ifelse(uni$Curriculum=="MOE" & uni$Physics >89,4.00,ifelse(uni$Curriculum=="MOE" & uni$Physics >79,3.00,ifelse(uni$Curriculum=="MOE" & uni$Physics >69,2.00,ifelse(uni$Curriculum=="MOE" & uni$Physics >59,1.00, 0.00)))))))))))))))))))))))))))))))))))))))))))))))
}  

#ROUNDING OFF TO 2 DECIMAL PLACES
uni <- uni %>% 
  mutate_if(is.numeric, round, digits = 2)

str(uni)
```

#CONVERTING PLACEMENT SCORES TO 4.0 SCALE 
```{r Placement score conversion }

uni<-uni%>%
  mutate(MPE=(Math.Placement.Exam.Score/30)*4,PPE=((Physics.Placement.Score/30)*4),
         EPE =(Total/9)*4)


#ROUNDING OFF TO 2 DECIMAL PLACES
uni <- uni %>% 
  mutate_if(is.numeric, round, digits = 2)

str(uni)
#uni%>%
#  filter(Curriculum == "Others")
```

# UNDERGRADUATE GPA SCORE ENGNEERING
```{r RITgpa engineering}

colnames(uni)[3]<-"2014.GPA"
colnames(uni)[4]<-"2015.GPA"
colnames(uni)[5]<-"2016.GPA"
colnames(uni)[6]<-"2017.GPA"
colnames(uni)[7]<-"2018.GPA"

uni<-uni%>%
  mutate(First.Yr = ifelse(Application.Term %in% c("2141","2145"),`2014.GPA`,ifelse(Application.Term %in% c("2151","2155"),`2015.GPA`,ifelse(Application.Term %in% c("2161","2165"),`2016.GPA`,ifelse(Application.Term %in% c("2171","2175"),`2017.GPA`, `2018.GPA`)))), Second.Yr = ifelse(Application.Term %in% c("2141","2145"),`2015.GPA`,ifelse(Application.Term %in% c("2151","2155"),`2016.GPA`,ifelse(Application.Term %in% c("2161","2165"),`2017.GPA`,ifelse(Application.Term %in% c("2171","2175"),`2018.GPA`, 0.00)))), Third.Yr = ifelse(Application.Term %in% c("2141","2145"),`2016.GPA`,ifelse(Application.Term %in% c("2151","2155"),`2017.GPA`,ifelse(Application.Term %in% c("2161","2165"),`2018.GPA`,0.00))), Fourth.Yr = ifelse(Application.Term %in% c("2141","2145"),`2017.GPA`,ifelse(Application.Term %in% c("2151","2155"),`2018.GPA`,0.00)))

str(uni)
uni
```



```{r populating missing values}
#The missing values can be populated using mean

mean(uni$EnglishGrade, na.rm = TRUE)
EnglishGrade_NA <- which(is.na(uni$EnglishGrade)) 
uni$EnglishGrade[EnglishGrade_NA] <- mean(uni$EnglishGrade, na.rm = TRUE)

mean(uni$MathGrade, na.rm = TRUE)
MathGrade_NA <- which(is.na(uni$MathGrade))
uni$MathGrade[MathGrade_NA] <- mean(uni$MathGrade, na.rm = TRUE)

PhysicsGrade_NA <- which(is.na(uni$PhysicsGrade)) 
uni$PhysicsGrade[PhysicsGrade_NA] <- mean(uni$PhysicsGrade, na.rm = TRUE)

MPE_NA <- which(is.na(uni$MPE)) 
uni$MPE[MPE_NA] <- mean(uni$MPE, na.rm = TRUE)

PPE_NA <- which(is.na(uni$PPE)) 
uni$PPE[MPE_NA] <- mean(uni$PPE, na.rm = TRUE)

# Converting Curriculum & Current.Program to factor

#uni$Curriculum<-as.factor(uni$Curriculum)
#uni$Current.Program<-as.factor(uni$Current.Program)


#Replacing NA by 0
uni[is.na(uni)] <- 0
uni
###---------------------------------------------------------------
#ing columns 2014.GPA to 2018.GPA

X<-uni %>%
  filter(Application.Term %in% c('2141','2145','2151', '2155'))
#X[,c(46:48)]
str(X)
X
Curr_X<- X%>%
  group_by(Curriculum) %>%
  dplyr::summarize(count=n()) 
ggplot(Curr_X,aes(x=Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")+
   geom_label(aes(label=count),nudge_x=0,nudge_y=0, check_overlap=TRUE)+
 theme(legend.position= "none", axis.title = element_blank())+
  labs(title="Distribution of student records with non-cognitive predictors")+
  coord_flip()
FirstThird<-gather(X, key="FirstorThird", value="FirstThird.GPA", c(46:48), na.rm= FALSE )
FirstThird<-FirstThird%>%
  filter(FirstThird.GPA!=0)
FirstThird
  FirstThird%>%
  filter(FirstThird.GPA==0)
`%notin%` <- Negate(`%in%`)
#FirstThird<-FirstThird%>%
#  filter(`#` %notin% c('138', '150','8', '25', '149', '155','126','168','187'))
#uni%>%
 # filter()
```


```{r COLLEGE readiness}
##CALCULATIONS

unicoll<-uni%>%
  filter(Curriculum=="MOE")

nrow(unicoll) 


#Percentage of students below mean EPE
(nrow(unicoll)-(nrow(unicoll%>%
  filter(EPE>mean(uni$EPE)))))/nrow(unicoll)
mean(unicoll$EPE)
mean(unicoll$MPE)
mean(unicoll$PPE)
#Percentage of students below mean MPE
(nrow(unicoll)-(nrow(unicoll%>%
  filter(unicoll$MPE>mean(uni$MPE)))))/nrow(unicoll)



#Percentage of students below mean PPE
(nrow(unicoll)-nrow(unicoll%>%
  filter(unicoll$PPE>mean(uni$PPE, na.rm = TRUE))))/nrow(unicoll)
  
mean(uni$EPE, na.rm = TRUE)
mean(uni$MPE, na.rm = TRUE)
mean(uni$PPE, na.rm = TRUE)

#Percentage less than average 

```


```{r GPA evolution}


#Create a custom color scale
library(RColorBrewer)
myColors <- brewer.pal(8,"Set1")
names(myColors) <- levels(FirstThird$Curriculum)
colScale <- scale_colour_manual(name = "Curriculum",values = myColors)


##75th percentile line
p<-ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum, group = Curriculum))+
 geom_point(aes(group= NULL))+
   labs(title="GPA EVOLUTION", 
       subtitle="75th PERCENTILE LINE",
       x="YEAR OF STUDY",
       y="GPA", ymin=2.5, ymax=3.5)+
  theme(legend.position="top")+
  stat_summary(geom = 'line', fun.y = quantile, fun.args = list(probs = 0.75))
#  scale_colour_brewer(palette = "Paired")

#p+ scale_color_brewer(palette="Dark2")
p+scale_color_manual(values=c("red", "blue", "forestgreen","aquamarine","burlywood3","violet","darkorange","lightcoral"))

str(FirstThird)
##Mean trend line
q<-ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum, group = Curriculum))+
 geom_point(aes(group= NULL))+
   labs(title="GPA EVOLUTION", 
       subtitle="MEAN LINE",
       x="YEAR OF STUDY",
       y="GPA")+
  theme(legend.position="top")+ coord_cartesian(ylim = c(1, 4))+
  stat_summary(geom = 'line', fun.y = mean)
#  scale_colour_brewer(palette = "Paired")
q+ coord_cartesian(ylim = c(1, 4))
#q+ scale_color_brewer(palette="Set2")
q+ scale_color_manual(values=c("red", "blue", "forestgreen","aquamarine","burlywood3","violet","darkorange","lightcoral"))
q + theme(
plot.title = element_text(size=14, face="bold"),
axis.title.x = element_text(size=10, face="bold"),
axis.title.y = element_text(size=10, face="bold"),
legend.text =  element_text( size=10, face="bold"),legend.position="top",
#legend.title = element_blank(),axis.title.x=element_blank(),
   #     axis.text.x=element_blank(),
    #    axis.ticks.x=element_blank()
)



#Variability Comparison

p<-ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum, group = Curriculum))+
 geom_boxplot(aes(group= NULL))+
   labs(title="GPA EVOLUTION", 
       subtitle="VARIABILITY COMPARISON",
       x="YEAR OF STUDY",
       y="GPA")+
  theme(legend.position="top")
#  stat_summary(geom = 'line',fun.y = median)+
#  scale_colour_brewer(palette = "Paired")


p+scale_color_manual(values=c("red", "blue", "forestgreen","aquamarine","burlywood3","violet","darkorange","lightcoral"))



##CALCULATIONS

Mean_first<-FirstThird%>%
  filter(Curriculum=="SABIS" & FirstorThird == "First.Yr")

Mean_second<-FirstThird%>%
  filter(Curriculum=="SABIS" & FirstorThird == "Second.Yr")


Mean_third<-FirstThird%>%
  filter(Curriculum=="SABIS" & FirstorThird == "Third.Yr")

mean(Mean_first$PPE)
#Change from 1st to 2nd year
(mean(Mean_second$FirstThird.GPA)-mean(Mean_first$FirstThird.GPA))/mean(mean(Mean_first$FirstThird.GPA))
#Change from 2nd to 3rd
(mean(Mean_third$FirstThird.GPA)-mean(Mean_second$FirstThird.GPA))/mean(mean(Mean_second$FirstThird.GPA))
#Change from 1st to 3rd
(mean(Mean_third$FirstThird.GPA)-mean(Mean_first$FirstThird.GPA))/mean(mean(Mean_first$FirstThird.GPA))

summary(Mean_first$FirstThird.GPA)
IQR(Mean_first$FirstThird.GPA)
summary(Mean_second$FirstThird.GPA)
summary(Mean_third$FirstThird.GPA)


ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum, group = Curriculum))+
 geom_point(aes(group= NULL))+
  stat_summary(geom = 'line', fun.y = mean)

geom_smooth(method=lm , se=FALSE)
#facet_wrap(~Curriculum)

ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum , group = Curriculum))+
 geom_quantile(aes(color= Curriculum))+
#  geom_smooth(method=lm , color= 'red', fill="#69b3a2", se=TRUE)
facet_wrap(~Curriculum)


ggplot(FirstThird, aes(x = FirstorThird, y= FirstThird.GPA,fill = Curriculum, group= Curriculum, color = Curriculum)) +
  geom_dotplot(binaxis="y",stackgroups = TRUE, method = "histodot",binpositions="all",aes(group = NULL))+
  stat_summary(geom = 'line', fun.y = mean)
#+
 # facet_wrap(~Curriculum)


ggplot(data = FirstThird, aes(FirstorThird, y=FirstThird.GPA, color= Curriculum, group = Curriculum))+
 geom_boxplot(aes(group = NULL))+
  stat_summary(geom = 'line', fun.y = mean, position = position_dodge(0.75))+
# stat_summary(geom = 'line', fun.y = quantile, fun.args = list(probs = 0.5), position = position_dodge(0.75))+
theme(axis.text.x = element_text(angle=65, vjust=0.6)) + 
  labs(title="Box plot", 
       subtitle="GPA evolution",
       x="Year of study",
       y="GPA")


```




```{r curriculum comparison}
uni

ggplot(uni, aes(x=MathGrade,y=Curriculum))+
  geom_count(color=Curriculum) 


Curr_unibe<- unibe%>%
  group_by(Curriculum) %>%
  summarize(count=n()) 
ggplot(Curr_unibe,aes(x=Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")+
   geom_label(aes(label=count),nudge_x=0,nudge_y=0, check_overlap=TRUE)+
 theme(legend.position= "none", axis.title = element_blank())+
  labs(title="Distribution of student records with non-cognitive predictors")+
  coord_flip()

warn(ggplot(data=uni) +
  geom_bar(mapping = aes(x= MathGrade, fill = Curriculum )) +
  coord_flip()+
  labs(y="No.of records")+
    ylim("")+
 #   coord_flip()+
 facet_wrap(~Curriculum))
summary(uni)
uni%>%
  filter(Curriculum == 'MOE'& PhysicsGrade <1.81)
f
summary(f$EPE)
summary(f$MPE)
summary(f$PPE)

```

#SCORING SURVEY RESULTS OF GRIT & SELF-CONTROL 
```{r non-cognitive skill Numerisation}

 uni$Grit_1<- ifelse(uni$Grit_1 == "Very much like me", 5, ifelse(uni$Grit_1 == "Mostly like me",4, ifelse(uni$Grit_1 == "Somewhat like me",3,ifelse(uni$Grit_1 == "Not much like me" ,2,ifelse(uni$Grit_1 == "Not like me at all",1,0)))))
 uni$Grit_4<- ifelse(uni$Grit_4 == "Very much like me", 5, ifelse(uni$Grit_4 == "Mostly like me",4, ifelse(uni$Grit_4 == "Somewhat like me",3,ifelse(uni$Grit_4 == "Not much like me" ,2,ifelse(uni$Grit_4 == "Not like me at all",1,0)))))
 uni$Grit_6<- ifelse(uni$Grit_6 == "Very much like me", 5, ifelse(uni$Grit_6 == "Mostly like me",4, ifelse(uni$Grit_6 == "Somewhat like me",3,ifelse(uni$Grit_6 == "Not much like me" ,2,ifelse(uni$Grit_6 == "Not like me at all",1,0)))))
 uni$Grit_9<- ifelse(uni$Grit_9 == "Very much like me", 5, ifelse(uni$Grit_9 == "Mostly like me",4, ifelse(uni$Grit_9 == "Somewhat like me",3,ifelse(uni$Grit_9 == "Not much like me" ,2,ifelse(uni$Grit_9 == "Not like me at all",1,0)))))
 uni$Grit_10<- ifelse(uni$Grit_10 == "Very much like me", 5, ifelse(uni$Grit_10 == "Mostly like me",4, ifelse(uni$Grit_10 == "Somewhat like me",3,ifelse(uni$Grit_10 == "Not much like me" ,2,ifelse(uni$Grit_10 == "Not like me at all",1,0)))))
 uni$Grit_12<- ifelse(uni$Grit_12 == "Very much like me", 5, ifelse(uni$Grit_12 == "Mostly like me",4, ifelse(uni$Grit_12 == "Somewhat like me",3,ifelse(uni$Grit_12 == "Not much like me" ,2,ifelse(uni$Grit_12 == "Not like me at all",1,0)))))
 uni$Grit_2<- ifelse(uni$Grit_2 == "Very much like me", 1, ifelse(uni$Grit_2 == "Mostly like me",2, ifelse(uni$Grit_2 == "Somewhat like me",3,ifelse(uni$Grit_2 == "Not much like me",4,ifelse(uni$Grit_2 == "Not like me at all",5,0)))))
 uni$Grit_3<- ifelse(uni$Grit_3 == "Very much like me", 1, ifelse(uni$Grit_3 == "Mostly like me",2, ifelse(uni$Grit_3 == "Somewhat like me",3,ifelse(uni$Grit_3 == "Not much like me",4,ifelse(uni$Grit_3 == "Not like me at all",5,0)))))
 uni$Grit_5<- ifelse(uni$Grit_5 == "Very much like me", 1, ifelse(uni$Grit_5 == "Mostly like me",2, ifelse(uni$Grit_5 == "Somewhat like me",3,ifelse(uni$Grit_5 == "Not much like me",4,ifelse(uni$Grit_5 == "Not like me at all",5,0)))))
 uni$Grit_7<- ifelse(uni$Grit_7 == "Very much like me", 1, ifelse(uni$Grit_7 == "Mostly like me",2, ifelse(uni$Grit_7 == "Somewhat like me",3,ifelse(uni$Grit_7 == "Not much like me",4,ifelse(uni$Grit_7 == "Not like me at all",5,0)))))
 uni$Grit_8<- ifelse(uni$Grit_8 == "Very much like me", 1, ifelse(uni$Grit_8 == "Mostly like me",2, ifelse(uni$Grit_8 == "Somewhat like me",3,ifelse(uni$Grit_8 == "Not much like me",4,ifelse(uni$Grit_8 == "Not like me at all",5,0)))))
 uni$Grit_11<- ifelse(uni$Grit_11 == "Very much like me", 1, ifelse(uni$Grit_11 == "Mostly like me",2, ifelse(uni$Grit_11 == "Somewhat like me",3,ifelse(uni$Grit_11 == "Not much like me",4,ifelse(uni$Grit_11 == "Not like me at all",5,0)))))

uni$SelfControl_4<-ifelse(uni$SelfControl_4 == "Very much like me", 5, ifelse(uni$SelfControl_4 == "Mostly like me",4, ifelse(uni$SelfControl_4 == "Somewhat like me",3,ifelse(uni$SelfControl_4 == "Not much like me" ,2,ifelse(uni$SelfControl_4 == "Not like me at all",1,0)))))
uni$SelfControl_5<-ifelse(uni$SelfControl_5 == "Very much like me", 5, ifelse(uni$SelfControl_5 == "Mostly like me",4, ifelse(uni$SelfControl_5 == "Somewhat like me",3,ifelse(uni$SelfControl_5 == "Not much like me" ,2,ifelse(uni$SelfControl_5 == "Not like me at all",1,0)))))
 uni$SelfControl_6<-ifelse(uni$SelfControl_6 == "Very much like me", 5, ifelse(uni$SelfControl_6 == "Mostly like me",4, ifelse(uni$SelfControl_6 == "Somewhat like me",3,ifelse(uni$SelfControl_6 == "Not much like me" ,2,ifelse(uni$SelfControl_6 == "Not like me at all",1,0)))))

uni$SelfControl_1<- ifelse(uni$SelfControl_1 == "Very much like me", 1, ifelse(uni$SelfControl_1 == "Mostly like me",2, ifelse(uni$SelfControl_1 == "Somewhat like me",3,ifelse(uni$SelfControl_1 == "Not much like me",4,ifelse(uni$SelfControl_1 == "Not like me at all",5,0)))))
uni$SelfControl_2<- ifelse(uni$SelfControl_2 == "Very much like me", 1, ifelse(uni$SelfControl_2 == "Mostly like me",2, ifelse(uni$SelfControl_2 == "Somewhat like me",3,ifelse(uni$SelfControl_2 == "Not much like me",4,ifelse(uni$SelfControl_2 == "Not like me at all",5,0)))))
uni$SelfControl_3<- ifelse(uni$SelfControl_3 == "Very much like me", 1, ifelse(uni$SelfControl_3 == "Mostly like me",2, ifelse(uni$SelfControl_3 == "Somewhat like me",3,ifelse(uni$SelfControl_3 == "Not much like me",4,ifelse(uni$SelfControl_3 == "Not like me at all",5,0)))))
uni$SelfControl_7<- ifelse(uni$SelfControl_7 == "Very much like me", 1, ifelse(uni$SelfControl_7 == "Mostly like me",2, ifelse(uni$SelfControl_7 == "Somewhat like me",3,ifelse(uni$SelfControl_7 == "Not much like me",4,ifelse(uni$SelfControl_7 == "Not like me at all",5,0)))))
uni$SelfControl_8<- ifelse(uni$SelfControl_8 == "Very much like me", 1, ifelse(uni$SelfControl_8 == "Mostly like me",2, ifelse(uni$SelfControl_8 == "Somewhat like me",3,ifelse(uni$SelfControl_8 == "Not much like me",4,ifelse(uni$SelfControl_8 == "Not like me at all",5,0)))))
uni$SelfControl_9<- ifelse(uni$SelfControl_9 == "Very much like me", 1, ifelse(uni$SelfControl_9 == "Mostly like me",2, ifelse(uni$SelfControl_9 == "Somewhat like me",3,ifelse(uni$SelfControl_9 == "Not much like me",4,ifelse(uni$SelfControl_9 == "Not like me at all",5,0)))))
uni$SelfControl_10<- ifelse(uni$SelfControl_10 == "Very much like me", 1, ifelse(uni$SelfControl_10 == "Mostly like me",2, ifelse(uni$SelfControl_10 == "Somewhat like me",3,ifelse(uni$SelfControl_10 == "Not much like me",4,ifelse(uni$SelfControl_10 == "Not like me at all",5,0)))))
```


```{r unibe correlations}
#(((Math.Placement.Exam.Score/30)*100)/20)-1,PPE=(((Physics.Placement.Score/30)*100)/20)-1,
#         EPE =(((Total/9)*100)/20)-1)
#unibe
unibe<-uni %>%
  filter(Grit_1 != 0)
unibe<-unibe%>%
  mutate(GritScale = (((Grit_1+Grit_2+Grit_3+Grit_4+Grit_5+Grit_6+Grit_7+Grit_8+Grit_9+Grit_10)/12)/5)*4, SelfControlScale = (((SelfControl_1+SelfControl_2+SelfControl_3+SelfControl_4+SelfControl_5+SelfControl_6+SelfControl_7+SelfControl_8+SelfControl_9+SelfControl_10)/10)/5)*4)

#unibe$GritScale<- (uni$GritScale/5)*4
#unibe$SelfControlScale<- (uni$SelfControlScale/5)*4
str(unibe)
unibe[,c(3:7,15,17,19, 21:42)]
unibe<-unibe[,-c(3:7,15,17,19,21:42)]
unibe <- unibe %>% 
  mutate_if(is.numeric, round, digits = 2)
uni_withGrit<-uni
#uni<-uni_withGrit
uni[,c(21:42)]
uni<-uni[,-c(21:42)]
str(uni)
#MULTIPLE LINEAR REGRESSION MODEL TO ASS VARIABLE LINEARITIES
uni_lm<-lm(cgpa~MPE+PPE+EPE+ MathGrade+EnglishGrade+PhysicsGrade, data=uni)
summary(uni_lm)

library(modelr)
rsquare(uni_lm, data=uni)
#glance(uni_lm)
#Residuals:
#     Min       1Q   Median       3Q      Max 
#-1.54110 -0.32348  0.03976  0.35182  1.83819 

#Coefficients:

#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)   0.09936    0.29653   0.335  0.73791    
#MPE           0.34713    0.06913   5.021 1.13e-06 ***
#PPE           0.07939    0.08391   0.946  0.34523    
#EPE           0.32883    0.10618   3.097  0.00223 ** 
#MathGrade     0.12609    0.04491   2.808  0.00548 ** 
#EnglishGrade  0.13196    0.05232   2.522  0.01243 *  
#PhysicsGrade -0.02786    0.04089  -0.681  0.49652 
#---
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


#Residual standard error: 0.5396 on 202 degrees of freedom
#Multiple R-squared:  0.4421,	Adjusted R-squared:  0.4255 
#F-statistic: 26.68 on 6 and 202 DF,  p-value: < 2.2e-16
  
  
uniFirst_lm<-lm(First.Yr~MPE+PPE+EPE+ MathGrade+EnglishGrade+PhysicsGrade, data=uni)
str(uni)
summary(uniFirst_lm)



#Coefficients:
#             Estimate Std. Error t value Pr(>|t|)    
#(Intercept)   0.71688    0.37205   1.927   0.0554 .  
#MPE           0.38294    0.08674   4.415 1.65e-05 ***
#PPE          -0.04503    0.10529  -0.428   0.6693    
#EPE           0.16513    0.13323   1.239   0.2166    
#MathGrade     0.13743    0.05635   2.439   0.0156 *  
#EnglishGrade  0.13773    0.06564   2.098   0.0371 *  
#PhysicsGrade -0.02982    0.05131  -0.581   0.5618    
#---
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#Residual standard error: 0.677 on 202 degrees of freedom
#Multiple R-squared:  0.2725,	Adjusted R-squared:  0.2509 
#F-statistic: 12.61 on 6 and 202 DF,  p-value: 4.574e-12
  
str(unibe)
```



```{r gritscale & self control impact }
#unibe$GritScale<-as.factor(unibe$GritScale)


#Writing the new file into csv
write.csv(unibe, file = "Non-Cognitive.csv", row.names = F)


#GritScale Vs Curriculum  
ggplot(data = unibe, aes(x=Curriculum, y=GritScale, color= Curriculum, group = Curriculum))+
 geom_point(aes(group= NULL))+
   labs(title="Grit Scale Vs Curriculum",
       x="",
       y="")+
  coord_flip()
#unibe<-unibe[,c(3,10:15,20,21)]
#GritScale Vs GPA
Y<- unibe %>%
  filter(Curriculum == "British")
ggplot(data = Y, aes(x=cgpa, y=GritScale, color= Curriculum))+
 geom_point()+
   labs(title="GritScale Vs GPA",
       x="GPA",
       y="GritScale")+
  scale_colour_brewer(palette = "Paired")
str(unibe)

#Distribution bar chart
Curr_unibe<- unibe%>%
  group_by(Curriculum) %>%
  dplyr::summarize(count=n()) 
ggplot(Curr_unibe,aes(x=Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")+
   geom_label(aes(label=count),nudge_x=0,nudge_y=0, check_overlap=TRUE)+
 theme(legend.position= "none", axis.title = element_blank())+
  labs(title="Distribution of student records with non-cognitive predictors")+
  coord_flip()

#FacetWrap
ggplot(unibe, aes(x= GritScale, y=cgpa, fill= Curriculum))+
  geom_point(aes(col=Curriculum))+
   theme(legend.position= "none")+
 #labs(title="CGPA vs Grit Scale")+
  facet_wrap(~Curriculum)

ggplot(unibe, aes(x= SelfControlScale, y=cgpa, fill= Curriculum))+
  geom_point(aes(col=Curriculum))+
   theme(legend.position= "none")+
# labs(title="CGPA vs SelfControl Scale")+
  facet_wrap(~Curriculum)

#unibe%>%
#  filter(cgpa)
#Checking linearity
linear<- lm(cgpa~GritScale+SelfControlScale+MPE+PPE+EPE+MathGrade+EnglishGrade+PhysicsGrade ,unibe)
#for the 4 plots
plot(linear)
plot(linear, 4, id.n = 5)
metric<-augment(linear)
metric %>%
  top_n(3, wt = .cooksd)
str(unibe)
unibe[,c(3,10:15,20,21)]
library(GGally)
#Correlation matrix
unibe_cor<-unibe[,c(3,10:15,20,21)]
ggpairs(unibe_cor, title = "Correlation Matrix", aes(color ="blue"))

#Correlation matrix for British
unibe_cor<-unibe%>%
  filter(Curriculum=="British")
unibe_cor<-unibe_cor[,c(3,10:15,20,21)]

str(unibe_cor)
ggpairs(unibe_cor, title = "Correlation Matrix for British Curriculum", aes(color ="blue"))

Curr_unibe<- unibe%>%
  group_by(Curriculum) %>%
  summarize(count=n()) 
ggplot(Curr_unibe,aes(x=Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")
#Multiple linear regression
unibe_lm<-lm(cgpa~GritScale+SelfControlScale + MPE + EPE +MathGrade+ PPE+ EnglishGrade+ PhysicsGrade, data = unibe)

#Coefficients:
 #    (Intercept)         GritScale  SelfControlScale               MPE               EPE         MathGrade  
#     -0.504389          0.208435         -0.076347          0.268431          0.522093          0.147683  
#             PPE      EnglishGrade      PhysicsGrade  
 #       0.005613          0.138139         -0.011097  
        
```



```{r bucketing cgpa}
uni<-uni%>%
  mutate(GPA = ifelse(cgpa>3.7, 4,ifelse(cgpa>3.5, 3.5,ifelse(cgpa>3,3,ifelse(cgpa>2.5,2.5, ifelse(cgpa>2,2,1))))))
str(uni)
uni$GPA<-as.factor(uni$GPA)


#unisub<-uni[,-c(1:11,13,15,19:22)]
#str(unisub)
#unisub$GPA<-as.factor(unisub$GPA)
```

```{r removing raw columns already converted}
#Total, Math.Placement.Exam.Score, Physics.Placement.Score, English, math,Physics
uni[,c(10:12,15,17,19)]
uni<-uni[,-c(10:12,15,17,19)]
str(uni)

#Writing the new file into csv
write.csv(uni, file = "New_uni.csv", row.names = F)

#Statistical summary
library(pastecs)
stat.desc(uni[,c()])
```


```{r Variable importance Boruta}

#Boruta method

uni_boruta<-uni[,c(9,11,13,15:18,25)]
str(uni)
library(Boruta)
uni_boruta
boruta_output<- Boruta(First.Yr ~., data= na.omit(uni_boruta), doTrace=1)
names(boruta_output)

roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ]) 
# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 

#Recursive feature elimnation: a rigorous way to determine the important variables before you even feed them into a ML algo.

set.seed(100)
options(warn=-1)

subsets <- c(1:3, 10, 15, 18)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x=uni_boruta[, c(1:3, 4:7)], y=uni_boruta$GPA,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile
lmProfile$optVariables
str(uni)
```

```{r visualizations_correlation}
my_cols <- c("#00AFBB", "#E7B800", "#FC4E07", "blue", "red", "black", "green","yellow") 
pairs(uni[,c(8,16,18,20:23)], col = my_cols[uni$Curriculum])
uni

ggpairs(uni,columns= c(14,8,16,18,20:23),  title="correlation matrix",               
        mapping= aes(colour = Curriculum), 
        lower = list(
       #   continuous = "smooth",
          combo = "facetdensity",
          mapping = aes(color = Curriculum)),
        upper = list(continuous = wrap("cor", size = 3, hjust=0.8)))
?ggpairs

#CORRELATION MATRIX FOR ALL PREDICTORS

ggpairs(uni,columns= c(8,12:17),  title="correlation matrix",               
        lower = list(
          continuous = wrap("cor", size = 3, hjust=0.8),
          combo = "facetdensity"),
    #      mapping = aes(color = Curriculum)),
        upper = list(continuous = "smooth",      mapping= aes(colour = Curriculum)))
  
ggcorr(d, method = c("everything", "pearson"))
d<-uni[,c(8,12:17)]
d
uni$Curriculum
str(uni)
```


```{r correlation for each curriculum}
str(uni)
uni_Ind<-uni%>%
  filter(Curriculum=="Indian")
uni_Ind<-uni_Ind[,c(8,16, 18, 20:23)]
uni_Ind
ggpairs(uni_Ind, title = "Correlation for Indian Curriculum", aes(color ="blue"))
#cor(uni_Ind)
uni_Ind
str(uni)

uni_Ame<-uni%>%
  filter(Curriculum=="American")
uni_Ame<-uni_Ame[,c(8,16, 18, 20:23)]

ggpairs(uni_Ame, title = "Correlation for American Curriculum", aes(color = "lightblue2"))



uni_Brit<-uni%>%
  filter(Curriculum=="British")
uni_Brit<-uni_Brit[,c(8,16, 18, 20:23)]

ggpairs(uni_Brit, title = "Correlation for British Curriculum", aes(color="red"))



uni_Sab<-uni%>%
  filter(Curriculum=="SABIS")
uni_Sab<-uni_Sab[,c(8,16, 18, 20:23)]

ggpairs(uni_Sab, title = "Correlation  for SABIS Curriculum", aes(color="blue"))


uni_Moe<-uni%>%
  filter(Curriculum=="MOE")
uni_Moe<-uni_Moe[,c(8,16, 18, 20:23)]

ggpairs(uni_Moe, title = "Correlation for MOE Curriculum", aes(color= "green"))



uni_Ib<-uni%>%
  filter(Curriculum=="IB")
uni_Ib<-uni_Ib[,c(8,16, 18, 20:23)]

ggpairs(uni_Ib, title = "Correlation for IB",aes(color ="green"))


uni_Afr<-uni%>%
  filter(Curriculum=="African")
uni_Afr<-uni_Afr[,c(8,16, 18, 20:23)]
uni_Afr
ggpairs(uni_Afr, title = "Correlation for African Curriculum", aes(color= "cornsilk4"))



uni_Oth<-uni%>%
  filter(Curriculum=="Others")
uni_Oth<-uni_Oth[,c(8,16, 18, 20:23)]

ggpairs(uni_Oth, title = "Correlation for all other curricula", aes(color= "burlywood4"))

```

```{r boxplot}
library(ggplot2)
theme_set(theme_bw())

# plot
g <- ggplot(uni, aes(Curriculum, cgpa))
g + geom_boxplot(aes(fill=Curriculum)) + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + 
  labs(title="Box plot + Dot plot", 
       subtitle="GPA vs Curriculum: Each dot represents 1 row in source data",
       caption="Source: RIT Dubai",
       x="",
       y="GPA")



```

```{r correlationplot}
library(corrplot)
unicorr<- uni[,c(8,16,18,20:23)]
str(uni)
#Getting the summary for the different variables
summary(unicorr)

# calculate correlations
correlations <- cor(uni[,c(8,16,18,20:23)])
str(unicorr)
#unicorr<-unicorr[,colSums(is.na(unicorr))==0]
unicorr
#correlations <- cor(unicorr)
#Inorder to use only the rows which are complete
correlations <- cor(unicorr,use ="complete.obs")
correlations

# create correlation plot
corrplot(correlations, method="circle", na.label = "NA")

#Heat map
palette = colorRampPalette(c("green", "white", "red")) (10)
heatmap(x = correlations, col = palette, symm = TRUE)

# Generating table of correlation coefficients (the correlation matrix) and another table of the p-values.
#install.packages("Hmisc")
library("Hmisc")
unircorr = rcorr(as.matrix(unicorr))
#Correlation Matrix table with coefficients
unicorr.coeff = unircorr$r
unicorr.coeff
#Table with p-values
unicorr.p = unircorr$P
unicorr.p

#Pairs
ggpairs(unicorr)
```



```{r distibution (NOT USED), echo= FALSE}
#install.packages("GGally")
library(ggplot2)
library(GGally)
uni$Curriculum<-as.factor(uni$Curriculum)
str(uni)

#uni[,c(1:7,9,10,18:21)]

unisub<-uni[,-c(1:7,9,10,18:21)]
str(unisub)

#ggpairs(unisub, cardinality_threshold = 19, use= "complete.obs")
ggpairs(unisub, cardinality_threshold = 19, columns = 1:8)
uni_big<-uni %>%
#  filter(Curriculum %in% c("Indian","American","British","SABIS","MOE"))%>%
    group_by(Curriculum) %>%
  summarize(count=n())%>%
  filter(count>10)

warn(q<-ggplot(uni_big,aes(x= Curriculum,y=count, fill = Curriculum)) +
  geom_bar(stat="identity")+
  geom_label(aes(label=count),nudge_x=0,nudge_y=1, check_overlap=TRUE)+
   theme(legend.position="none")+
  labs(x=" ",y=" ", title ="HS curriculum distribution"))
q
```
This shows the distribution of students from High School curricula with more than 10 records in RIT.
The exploration will thus be more valid for the follwoing HS curricula: American, British, Indian, IB, MOE and SABIS.

#Furthur exploration

```{r First yr plot(NOT USED)}
str(uni)
FirstYrplot<-dplyr::filter(uni, First.Yr > 0.00 & (Curriculum == "Indian" | Curriculum == "American"| Curriculum == "British" | Curriculum == "SABIS" | Curriculum == "IB"))
warn(ggplot(data=FirstYrplot) +
  geom_bar(mapping = aes(x= First.Yr, fill =  MathGrade)) +
 # coord_flip()+
  labs(y="First year GPA", caption = "based on data from NYC311", title= "1st year gpa Distribution ")+
    ylim("")+
facet_wrap(~Curriculum))


 

```

#DATA SPLITTING
```{r splitting the data into test and train}
uni[,-c(1:7,9:11,18:22)]
str(uni)
unisub<-uni[,-c(1:7,9:11,18:22)]
str(unisub)
#unisub<-unisub[,-2]
dim(unisub)

library(caTools)
set.seed(1)
#train_data
#sample=sample.split(unisub$cgpa,SplitRatio = 0.80)
train_data=subset(unisub[0:167,])
test_data=subset(unisub[168:209,])

train_data

test_data
```
167 rows of train and 42 rows in test


#Algorithms on the dataset
The dataset requires regression to predict the future GPA of a student given RIT placement scores, HS curriculum and HS scores.

```{r Multiple linear regression}
#LM with curriculum
rm(regressor)
regressor=lm(cgpa~.,train_data)
summary(regressor)
#EPE has p value 0.00612, MPE has pvale 4.86e-05 and MathGrade has pvalue 0.00723
cc<-regressor$coefficients
(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))
cgpa_pred=predict(regressor,test_data)
cgpa_resid = residuals(regressor,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
sqrt(mean(regressor$residuals^2))
head(final_data,6)
final_data
d<-test_data
#RMSE
RMSE = (sum((test_data$cgpa - cgpa_pred)^2) / nrow(test_data)) ^ 0.5
RMSE
#Rsquared
rsquare(regressor, test_data)
#rmse(test_data$cgpa,cgpa_pred)

# model1 - EPE & MPE
model1=lm(cgpa~EPE+MPE+PPE,train_data)
summary(model1)
cc<-model1$coefficients
(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))
glance(model1) %>%
  dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)
rsquare(model1, test_data)
sqrt(mean(model1$residuals^2))

cgpa_pred=predict(model1,test_data)
cgpa_resid = residuals(model1,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)
d<-test_data

#RMSE
RMSE = (sum((test_data$cgpa - cgpa_pred)^2) / nrow(test_data)) ^ 0.5
RMSE
#Rsquared
rsquare(model1, test_data)
#rmse(test_data$cgpa,cgpa_pred)

# model2 - MPE,EPE & PPE
model2=lm(cgpa~EPE+MPE+PPE,train_data)

summary(model2)

cc<-model2$coefficients
(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

glance(model2) %>%
  dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)
rsquare(model2, test_data)
sqrt(mean(model2$residuals^2))

cgpa_pred=predict(model2,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)


# model3 - MPE
model3=lm(cgpa~MPE,train_data)
summary(model3)
cc<-model3$coefficients

(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

glance(model3) %>%
  dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)
rsquare(model3, test_data)
sqrt(mean(model3$residuals^2))

cgpa_pred=predict(model3,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)

# model4 - MPE,PPE,EPE,MathGrade
model4=lm(cgpa~MPE+PPE+EPE+MathGrade,train_data)
summary(model4)
cc<-model4$coefficients

(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

glance(model4) %>%
  dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)
rsquare(model4, test_data)
sqrt(mean(model4$residuals^2))

cgpa_pred=predict(model4,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)
#RMSE
RMSE = (sum((test_data$cgpa - cgpa_pred)^2) / nrow(test_data)) ^ 0.5
RMSE
#Rsquared
rsquare(model4, test_data)


#testing
model5=lm(cgpa~MPE+EPE+PhysicsGrade+MathGrade+EnglishGrade,train_data)
#0.4365
#model5=lm(cgpa~MPE+EPE+PPE+MathGrade+Curriculum,train_data)
summary(model5)


cc<-model5$coefficients
(eqn <- paste("Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

glance(model5) %>%
  dplyr::select(adj.r.squared, sigma, AIC, BIC, p.value)

cgpa_pred=predict(model5,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)

#R squared & RMSE
rsquare(model5, test_data)
sqrt(mean(model5$residuals^2))


#--------------------------------------------------------------------------
#Getting Accuracy
results <- data.frame(cbind(actual = test_data$cgpa, prediction = cgpa_pred))
results
#results<-data.frame(cbind(actual = datatest$cgpa, prediction = predict_testNN))

correlation_accuracy<-cor(results)
correlation_accuracy
#66.25%


#MinMaxAccuracy
min_max_accuracy <- mean(apply(results, 1, min) / apply(results, 1, max))  
min_max_accuracy 
#87.5% min_max accuracy 

#MAPE
mape <- mean(abs((results$prediction - results$actual))/results$actual)
mape
#14.5%

#Prediction Curve with MPE
ggplot(final_data,aes(x=MPE,y=cgpa_pred))+
geom_point(color="red")+
stat_smooth(method="lm")+
scale_x_continuous(name="Mathematics Placement Exam Score")+
scale_y_continuous(name="Prediction of GPA")+
ggtitle("Prediction Curve with MPE")

#plot(test_data$cgpa, cgpa_pred, col='green', pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main = "Multiple linear #regression model")+abline(0,1)


plot( cgpa_pred,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main = "Multiple linear regression model with all predictors")+abline(0,1)

plot( cgpa_pred,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main = "Multiple linear regression model with all predictors")+abline(lm(cgpa_pred~test_data$cgpa))

#plot(cgpa_pred,residuals,  pch=16, xlab = "Predicted GPA", ylab = "residuals", main = "Multiple linear regression model")+abline(0,1)

#Residuals
residuals=final_data$cgpa - final_data$cgpa_pred
#residuals=as.data.frame(residuals)
residuals
head(residuals,10)

d<-test_data
d$predicted<-cgpa_pred
d$residuals<-residuals

test_data
d
d[,c(1,8,9)]


#plottting

# COLOR
# High residuals (in abolsute terms) made more red on actual values.
ggplot(d, aes(x = MPE, y = cgpa)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = MPE, yend = predicted), alpha = .2) +

  # > Color adjustments made here...
  geom_point(aes(color = abs(residuals))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <

  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
```

```{r Neural Network}
set.seed(1)
unicur<-uni%>%
  filter(Curriculum %in% c("Indian","American","British","MOE","SABIS"))
unicur<-unicur[,-c(1:7,9:11,18:22)]
#unicur<-unisub
unicur<-unisub
str(unicur)
# Random sampling
samplesize = 0.80 * nrow(unicur)
set.seed(80)
index = sample( seq_len ( nrow ( unicur ) ), size = samplesize )

# Create training and test set
datatrain = unicur[ index, ]
datatest = unicur[ -index, ]

## Scale data for neural network

max = apply(unicur , 2 , max)
min = apply(unicur, 2 , min)
scaled = as.data.frame(scale(unicur, center = min, scale = max - min))

## Fit neural network 
rm(NN)
# install library
install.packages("neuralnet ")

# load library
library(neuralnet)

# creating training and test set
trainNN = scaled[index , ]
testNN = scaled[-index , ]
trainNN
# fit neural network
set.seed(2)
NN = neuralnet(cgpa ~MPE+EPE+PPE+MathGrade, trainNN, hidden = 3, linear.output = T )

# plot neural network
  plot(NN)

  testNN
## Prediction using neural network

predict_testNN = compute(NN, testNN[,c(2:7)])
predict_testNN
#Getting Accuracy
results <- data.frame(actual = datatest$cgpa, prediction = predict_testNN)
results
#results<-data.frame(cbind(actual = datatest$cgpa, prediction = predict_testNN))

correlation_accuracy<-cor(results)
correlation_accuracy

#With all curricula:69.7% accuracy
#with SAIS, MOE, American, British, Indian
#73.12%

#MinMaxAccuracy
min_max_accuracy <- mean(apply(results, 1, min) / apply(results, 1, max))  
min_max_accuracy 
#25% min_max accuracy 

#MAPE
mape <- mean(abs((results$prediction - results$actual))/results$actual)
mape
#74.8%

predict_testNN = (predict_testNN$net.result * 4)
#(max(unicur$cgpa) - min(unicur$cgpa))) + min(unicur$cgpa)
 min(unicur$cgpa)
predict_testNN

plot(datatest$cgpa, predict_testNN, col='blue', pch=16, ylab = "Predicted GPA", xlab = "Actual GPA")+abline(0,1)

#Rsquared
rsquare(NN, datatest)

# Calculate Root Mean Square Error (RMSE)
RMSE.NN = (sum((datatest$cgpa - predict_testNN)^2) / nrow(datatest)) ^ 0.5
RMSE.NN
#0.63



```

```{r CART}
library(rpart)

rm(fit)
fit <- rpart(cgpa ~MPE+EPE+PPE+MathGrade,
 method="anova", data=train_data,control=rpart.control(minsplit=2, cp=0.027))

fit <- rpart(cgpa ~.,
 method="anova", data=train_data,control=rpart.control(minsplit=2, cp=0.027))

train_data

#plotting
plot(fit, uniform=TRUE, 
 main="Regression Tree for GPA")+
 text(fit, use.n=TRUE, cex = .6)

#Looking at cross validation error(xerror) reducing 
printcp(fit)

par(mfrow=c(1,2)) 
rsq.rpart(fit)


#Prediction
cart<-predict(fit, test_data, method = "anova")

car<-plot(cart,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Decision Tree model with MPE, PPE, EPE & MathGrade")+abline(0,1)
car
car<-plot(cart,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Decision Tree model with MPE, PPE, EPE & MathGrade")+abline(lm(cart~test_data$cgpa))

final_data=cbind(test_data,cart) #Combining Predicted Value to Original Data

#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)

results <- data.frame(actual = test_data$cgpa, prediction = cart)
results
correlation_accuracy<-cor(results)
correlation_accuracy
#55.3%

#MinMaxAccuracy
min_max_accuracy <- mean(apply(results, 1, min) / apply(results, 1, max))  
min_max_accuracy 
#84% min_max accuracy 

#MAPE
mape <- mean(abs((results$prediction - results$actual))/results$actual)
mape
#17.9%

#Rsquared
rsquare(fit, test_data)

#RMSE
RMSE.cart = (sum((test_data$cgpa - cart)^2) / nrow(test_data)) ^ 0.5
RMSE.cart
```

```{r random forest}
require(randomForest)
library(miscTools)
#set.seed(1)
#sample=sample.split(unisub$cgpa,SplitRatio = 0.80)
#trainset=subset(unisub,sample==TRUE)
#testset=subset(unisub,sample==FALSE)
#trainset
rm(rf)
rm(pred)
rf = randomForest(cgpa~., data  = train_data)
rf

pred = predict(rf, newdata=test_data)

final_data=cbind(test_data,pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)


#(r2 <- rSquared(test_data$cgpa, test_data$cgpa - pred))
# [1] 0.44
#(mse <- mean((test_data$cgpa - pred)^2))
# [1] 0.24


results <- data.frame(actual = test_data$cgpa, prediction = pred)
#MinMaxAccuracy
min_max_accuracy <- mean(apply(results, 1, min) / apply(results, 1, max))  
min_max_accuracy 
#84% min_max accuracy 

#MAPE
mape <- mean(abs((results$prediction - results$actual))/results$actual)
mape
#17.9%

#Rsquared
rsquare(rf, test_data)

#RMSE
RMSE.rf = (sum((test_data$cgpa - pred)^2) / nrow(test_data)) ^ 0.5
RMSE.rf



plot(pred,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Random Forest with MPE, PPE, EPE & MathGrade")+abline(0,1)

plot(pred,test_data$cgpa, pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Random Forest with MPE, PPE, EPE & MathGrade")+abline(lm(pred~test_data$cgpa))

p <- ggplot(aes(x=actual, y=pred),
  data=data.frame(actual=test_data$cgpa , pred=pred))
ran<-p + geom_point() +
	geom_abline(color="red") +
	ggtitle(paste("RandomForest Regression in R r^2=", r2, sep=""))



```
Call:
 randomForest(formula = cgpa ~ ., data = trainset) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 2

          Mean of squared residuals: 0.3115989
                    % Var explained: 40.4

```{r plotting model comparison}
plot( cgpa_pred,test_data$cgpa, col='green',pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main = "Multiple linear regression model with all predictors")+abline(0,1)
library(ggthemes)
library(tidyr)
library(ggplot2)
#CART
car<-plot(cart,test_data$cgpa,col='red', pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Decision Tree model with MPE, PPE, EPE & MathGrade")+abline(0,1)

#Random forest
plot(pred,test_data$cgpa, col='blue',pch=16, xlab = "Predicted GPA", ylab = "Actual GPA", main= "Random Forest with MPE, PPE, EPE & MathGrade")+abline(0,1)

Actual_GPA<-test_data$cgpa
Actual<-test_data$cgpa
MLM<-cgpa_pred
DT<-cart
RF<-pred
MEAN<-(MLM+DT+RF)/3

final_data=cbind(Actual_GPA,Actual,MLM,DT,RF, MEAN)
head(final_data)
write.csv(final_data,"modelcomparison.csv")

final_data<-as.data.frame(final_data)
#Actual_GPA<-as.character(Actual_GPA)
#Actual_GPA
Modelplot<-gather(final_data, key="Model", value="Predicted_GPA", c(2:5), na.rm= FALSE )
Modelplot

p<-ggplot(data = Modelplot, aes(Actual_GPA, y=Predicted_GPA, color= Model, group = Model))+
#  geom_line(aes(group=NULL))+
 geom_point(aes(group= NULL))+
   labs(title="ACTUAL GPA VS PREDICTED GPA ", 
       x="",
       y="Actual & Predicted GPA", ymin=2.0, ymax=4.0
+
     theme_stata())
p + theme(
plot.title = element_text(size=14, face="bold"),
#axis.title.x = element_text(size=10, face="bold"),
axis.title.y = element_text(size=10, face="bold"),
legend.text =  element_text( size=10, face="bold"),legend.position="top",
legend.title = element_blank(),axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()
)
#Average
Averageplot<-gather(final_data, key="Model", value="AveragePredicted_GPA", c(2,6), na.rm= FALSE )
Averageplot
p<-ggplot(data = Averageplot, aes(Actual_GPA, y=AveragePredicted_GPA, color= Model, group = Model))+
 geom_line(aes(group=NULL))+
 geom_point(aes(group= NULL))+
   labs(title="ACTUAL GPA VS AVERAGE of PREDICTED GPAs ", 
       x="",
       y="Actual GPA & Average of Predicted GPAs", ymin=2.0, ymax=4.0
 +  theme_stata())
p + theme(
plot.title = element_text(size=14, face="bold"),
#axis.title.x = element_text(size=10, face="bold"),
axis.title.y = element_text(size=10, face="bold"),
legend.text =  element_text( size=10, face="bold"),legend.position="top",
legend.title = element_blank(),axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()
)

```


```{r lm prediction for different curricula}
## for Indian Curricula
set.seed(1)
sample=sample.split(uni_Ind$cgpa,SplitRatio = 0.80)
train_data=subset(uni_Ind,sample==TRUE)
test_data=subset(uni_Ind,sample==FALSE)

#LM with curriculum
regressor=lm(cgpa~.,train_data)
summary(regressor)
#EPE has p value 0.0015 and MPE has pvale 4.86e-05

#Rerun model
new_model=lm(cgpa~MPE,train_data)
summary(new_model)

cgpa_pred=predict(new_model,test_data)
final_data=cbind(test_data,cgpa_pred) #Combining Predicted Value to Original Data
#write.csv(final_data,"lmmodel_data.csv")
head(final_data,6)

#Prediction Curve with MPE
ggplot(final_data,aes(x=MPE,y=cgpa_pred))+
geom_point(color="red")+
stat_smooth(method="lm")+
scale_x_continuous(name="Mathematics Placement Exam Score")+
scale_y_continuous(name="Prediction of GPA")+
ggtitle("Prediction Curve with MPE")


#Residuals
residuals=final_data$cgpa - final_data$cgpa_pred
residuals=as.data.frame(residuals)
residuals
head(residuals,10)
```
Call:
lm(formula = cgpa ~ ., data = train_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.01937 -0.21529  0.00193  0.17707  1.28502 

Coefficients: (1 not defined because of singularities)
             Estimate Std. Error t value Pr(>|t|)  
(Intercept)   1.90882    1.04973   1.818   0.0963 .
EnglishGrade       NA         NA      NA       NA  
MathGrade     0.23973    0.12819   1.870   0.0883 .
PhysicsGrade  0.06597    0.16235   0.406   0.6923  
MPE           0.58987    0.22744   2.594   0.0250 *
PPE          -0.12291    0.26233  -0.469   0.6486  

```{r Regression diagnostic plot_prediction models}
#Linear regression
linear_model<-with(uni,lm(cgpa~MathGrade+EnglishGrade+PhysicsGrade+Curriculum+MPE+PPE+EPE))
plot(linear_model)
linear_model1<-with(uni,lm(cgpa~MathGrade))
plot(linear_model1)
linear_model2<-with(uni,lm(cgpa~EnglishGrade))
plot(linear_model2)
linear_model3<-with(uni,lm(cgpa~PhysicsGrade))
plot(linear_model3)

linear_model4<-with(uni,lm(cgpa~Curriculum))
plot(linear_model4)


linear_model5<-with(uni,lm(cgpa~MPE))
plot(linear_model5)

linear_model6<-with(uni,lm(cgpa~PPE))
plot(linear_model6)

linear_model7<-with(uni,lm(cgpa~EPE))
plot(linear_model7)

linear_model<-with(uni,lm(cgpa~MathGrade+EnglishGrade+PhysicsGrade+Curriculum+MPE+PPE+EPE))
plot(linear_model3)

#+EnglishGrade+PhysicsGrade+Curriculum+MPE+PPE+EPE))
linear_model<-with(uni,lm(cgpa~MathGrade+EPE+MPE+PPE))
#linear_model<-with(unisub,lm(cgpa~.))
#linear_model<-with(train1,lm(cgpa~MPE))
summary(linear_model)
confint(linear_model)
#Dummy variable made for the categorical variable "Curriculum"
contrasts(uni$Curriculum)
library(broom)
linear_uni<- augment(linear_model)
head(linear_uni)
linear_uni <- uni %>%
  mutate(index = 1:nrow(linear_uni)) %>%
  select(index, everything(), -.se.fit, -.sigma)
head(linear_uni, 4)
##Diagonising the results using Residual plots
par(mfrow = c(2, 2)) 
plot(linear_model)
uni
#The plots demonstrate that the relationship is linear

#Fit plot
ggplot(data = train1 , aes(x = MathGrade , y = cgpa)) +
geom_point() +
stat_smooth(method = "lm", col = "dodgerblue3") +
theme(panel.background = element_rect(fill = "white"),
axis.line.x=element_line(),
axis.line.y=element_line()) +
ggtitle("Linear Model Fitted to Data")


```
# p value is low for 





```{r spot_algorithm CLASSIFICATION}
library(mlbench)
install.packages('caret', dependencies = TRUE)
library(caret)
seed<-7
metric <- "Accuracy"
#Repeated cross validation
control <- trainControl(method = "repeatedcv", repeats = 3)
train_data
# Linear Discriminant Analysis
train_data
set.seed(seed)
fit.lda <- train( GPA~., data=train_data, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control, na.action = na.exclude)
# Logistic Regression not possible as there are more than 2 classes

# GLMNET
set.seed(seed)
fit.glmnet <- train( GPA~., data=train_data, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control,na.action = na.exclude)
# SVM Radial
set.seed(seed)
fit.svmRadial <- train( GPA~., data=train_data, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE, na.action = na.exclude)
# kNN
set.seed(seed)
fit.knn <- train( GPA~., data=train_data, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control, na.action = na.exclude)
# Naive Bayes
set.seed(seed)
fit.nb <- train( GPA~., data=train_data, method="nb", metric=metric, trControl=control,na.action = na.exclude)
# CART
set.seed(seed)
fit.cart <- train( GPA~., data=train_data, method="rpart", metric=metric, trControl=control, na.action = na.exclude)
# C5.0
set.seed(seed)
fit.c50 <- train( GPA~., data=train_data, method="C5.0", metric=metric, trControl=control, na.action = na.exclude)
# Bagged CART
set.seed(seed)
fit.treebag <- train( GPA~., data=train_data, method="treebag", metric=metric, trControl=control, na.action = na.exclude)
# Random Forest
set.seed(seed)
fit.rf <- train( GPA~., data=train_data, method="rf", metric=metric, trControl=control,na.action = na.exclude)
# Stochastic Gradient Boosting (Generalized Boosted Modeling)
set.seed(seed)
fit.gbm <- train( GPA~., data=train_data, method="gbm", metric=metric, trControl=control, verbose=FALSE,na.action = na.pass)

# Model selection

results <- resamples(list(lda=fit.lda, glmnet=fit.glmnet,
	svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
	bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
# Table comparison
summary(results)
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)

# density plots of accuracy
scales <- list(x=list(relation="free"), y=list(relation="free"))
densityplot(results, scales=scales, pch = "|")


## Good options could be svm, lda, knn, gbm

# Prediction
# Linear regression
pred_svmRadial <- predict(fit.svmRadial, test_data, na.action=na.omit)

# Linear Discriminant Analysis
pred_lda <- predict(fit.lda, test_data)
# kNN
pred_knn <- predict(fit.knn, test_data)
# rf
pred_rf <- predict(fit.rf, test_data)
# bagging
pred_treebag <- predict(fit.treebag, test_data)
# c50
pred_c50 <- predict(fit.c50, test_data)

# Stochastic Gradient Boosting
pred_gbm <- predict(fit.gbm, test_data)


pred_lda
test_data$GPA

postResample(pred_lda,test_data$GPA)

confusionMatrix(pred_lda,test_data$GPA)
#Accuracy =42.47, Kappa = 27.09
confusionMatrix(pred_svmRadial,test_data$GPA)
# Accuracy =45.21, Kappa=29.89
confusionMatrix(pred_knn,test_data$GPA)
#Accuracy= 41.10, Kappa= 25.10
confusionMatrix(pred_rf,test_data$GPA)
confusionMatrix(pred_treebag,test_data$GPA)
confusionMatrix(pred_c50,test_data$GPA)
confusionMatrix(pred_gbm,test_data$GPA)


```

```{r no split spot}
##---------------------------------------------------------------------------------------------------------------

##CrOSS VALIDATION with whole UNISUB ( no data splitting)


#cross validation
control <- trainControl(method = "repeatedcv", number  = 10, repeats =3)

# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(GPA~., data=unisub, method="lda", metric=metric, preProcess=c("center", "scale"), trControl=control)
# Logistic Regression not possible as there are more than 2 classes

# GLMNET
set.seed(seed)
fit.glmnet <- train(GPA~., data=unisub, method="glmnet", metric=metric, preProcess=c("center", "scale"), trControl=control)
# SVM Radial
set.seed(seed)
fit.svmRadial <- train(GPA~., data=unisub, method="svmRadial", metric=metric, preProcess=c("center", "scale"), trControl=control, fit=FALSE, na.action=na.omit)
# kNN
set.seed(seed)
fit.knn <- train(GPA~., data=unisub, method="knn", metric=metric, preProcess=c("center", "scale"), trControl=control)
# Naive Bayes
set.seed(seed)
fit.nb <- train(GPA~., data=unisub, method="nb", metric=metric, trControl=control, preProcess=c("scale"))
# CART
set.seed(seed)
fit.cart <- train(GPA~., data=unisub, method="rpart", metric=metric, trControl=control,  preProcess=c("scale"))
# C5.0
set.seed(seed)
fit.c50 <- train(GPA~., data=unisub, method="C5.0", metric=metric, trControl=control,  preProcess=c("scale"))
# Bagged CART
set.seed(seed)
fit.treebag <- train(GPA~., data=unisub, method="treebag", metric=metric, trControl=control,  preProcess=c("scale"))
# Random Forest
set.seed(seed)
fit.rf <- train(GPA~., data=unisub, method="rf", metric=metric, trControl=control,  preProcess=c("scale"))
# Stochastic Gradient Boosting (Generalized Boosted Modeling)
set.seed(seed)
fit.gbm <- train(GPA~., data=unisub, method="gbm", metric=metric, trControl=control, verbose=FALSE,  preProcess=c("scale"))



# Model selection

results <- resamples(list(lda=fit.lda, glmnet=fit.glmnet,
	svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
	bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
# Table comparison
summary(results)

# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)

## Good options could be svm, lda, knn, gbm


# Prediction
# Linear regression
pred_svmRadial <- predict(fit.svmRadial, unisub)
# Decision Tree
pred_lda <- predict(fit.lda, unisub)
# Random Forest
pred_gbm <- predict(fit.gbm, unisub)
# kNN
pred_knn <- predict(fit.knn, unisub)
# CART
pred_cart <- predict(fit.cart, unisub)
# Random Forest
pred_rf <- predict(fit.rf, unisub)
# GLMNET
pred_glmnet <- predict(fit.glmnet, unisub)

pred_nb<- predict(fit.nb, unisub)

pred_c50<- predict(fit.c50, unisub)

pred_treebag<- predict(fit.treebag, unisub)



confusionMatrix(pred_lda,unisub$GPA)
#Accuracy =46.99, Kappa = 31.27
confusionMatrix(pred_svmRadial,unisub$GPA)
# Accuracy =47.54, Kappa=30.57
confusionMatrix(pred_gbm ,unisub$GPA)
#Accuracy = 53.01 , Kappa = 38.86
confusionMatrix(pred_knn,unisub$GPA)
#Accuracy= 51.37, Kappa= 37.46
confusionMatrix(pred_cart,unisub$GPA)
#Accuracy =45.9, Kappa = 28.21
confusionMatrix(pred_rf,unisub$GPA)
#Accuracy =96.17, Kappa = 95.14
confusionMatrix(pred_glmnet,unisub$GPA)
#Accuracy =46.99, Kappa = 30.24
confusionMatrix(pred_nb,unisub$GPA)
#Accuracy =47.54, Kappa = 32.9
confusionMatrix(pred_c50,unisub$GPA)
#Accuracy =68.85, Kappa = 60.17
confusionMatrix(pred_treebag,unisub$GPA)
#Accuracy =98.91, Kappa = 98.61


resamps<-resamples(list(svm=pred_svmRadial,lda=pred_lda, gbm=pred_gbm, knn=pred_knn))
```

```{r splitting the data into test and train}
#unisub<-uni[,-c(1:7,9:10,18:22)]
str(uni)
uni[,-c(1:7,9:10,18:22)]
uni[,c(1:7,9:15,17,19,24:27)]
unisub<-uni[,-c(1:7,9:15,17,19,24:27)]
str(unisub)
#unisub<-unisub[,-2]
dim(unisub)

library(caTools)
set.seed(1)
sample=sample.split(unisub$cgpa,SplitRatio = 0.80)
train_data=subset(unisub,sample==TRUE)
test_data=subset(unisub,sample==FALSE)
#unisub<-unisub[,-c(1)]
#unisub<-unisub[,c(1,4,6:8)]
#train_data<-unisub[1:150,]
train_data
#test_data<-unisub[151:183,]
test_data
```

```{r algorithms REGRESSION} 
library(caret)

modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames


#Repeated cross validation
control <- trainControl(method = "repeatedcv", repeats = 3)

#Linear regression:
set.seed(1)
model_lm <- train(cgpa~., data=train_data, trControl = control, method = "lm",tuneLength = 15)
# Decision Tree:
set.seed(1)
model_dt <- train(cgpa~., data=train_data, trControl = control, method = "rpart",tuneLength = 15)
# Random Forest:
set.seed(1)
model_rf <- train(cgpa~., data=train_data, trControl = control, method = "rf", tuneLength = 15)
# kNN
set.seed(1)
model_knn <- train(cgpa~., data=train_data, trControl = control, method = "knn", tuneLength = 15)
# NNET
set.seed(1)
model_nn <- train(cgpa~MPE+PPE+EPE+MathGrade, data=train_data, trControl = control, method = "nnet", tuneLength = 15)
# GBM
set.seed(1)
model_gbm <- train(cgpa~MPE+PPE+EPE+MathGrade, data=train_data, trControl = control, method = "gbm",tuneLength = 15)

ggplot(model_dt)

# Prediction
# Linear regression
pred_lm <- predict(model_lm, test_data)

#Rsquared
summary(model_lm)
rsquare(model_lm, test_data)

#RMSE
RMSE.lm = (sum((test_data$cgpa - pred_lm)^2) / nrow(test_data)) ^ 0.5
RMSE.lm

# Decision Tree
pred_dt <- predict(model_dt, test_data)
#Rsquared
rsquare(model_dt, test_data)
summary(model_dt)

#RMSE
RMSE.dt = (sum((test_data$cgpa - pred_dt)^2) / nrow(test_data)) ^ 0.5
RMSE.dt

# Random Forest
pred_rf <- predict(model_rf, test_data)

#Rsquared
rsquare(model_rf, test_data)
library(rsq)
rsq(model_rf)
summary(model_rf)$r.squared
adj.r.#RMSE
RMSE.rf = (sum((test_data$cgpa - pred_rf)^2) / nrow(test_data)) ^ 0.5
RMSE.rf

# kNN
pred_knn <- predict(model_knn, test_data)

#Rsquared
rsquare(model_knn, test_data)
#RMSE
RMSE.knn= (sum((test_data$cgpa - pred_knn)^2) / nrow(test_data)) ^ 0.5
RMSE.knn

# Neural NEt
pred_nn <- predict(model_nn, test_data)

#Rsquared
rsquare(model_nn, test_data)
#RMSE
RMSE.nn= (sum((test_data$cgpa - pred_nn)^2) / nrow(test_data)) ^ 0.5
RMSE.nn

# GBM
pred_gbm <- predict(model_gbm, test_data)

#Rsquared
rsquare(model_gbm, test_data)
#RMSE
RMSE.gbm= (sum((test_data$cgpa - pred_gbm)^2) / nrow(test_data)) ^ 0.5
RMSE.gbm
#---------------------------------------------------------------------

model_dt
pred_lm
table(pred_dt)
#library(pROC)
#library(randomForest)
#importance(pred_rf)
#auc(pred_rf,test$GPA)
resamps<-resamples(list(pred_rf,pred_knn))



#confusionMatrix(data=pred_lm, test1$cgpa)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
